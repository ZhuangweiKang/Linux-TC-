{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linux TC 带宽管理队列规则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考链接：\n",
    "1. [Linux TC 带宽管理队列规则](https://blog.csdn.net/qinyushuang/article/details/46611923)\n",
    "2. [HTB分层令牌桶](http://pipul.org/2016/04/hierarchical-token-bucket/)\n",
    "3. [Linux TC(Traffic Control) 简介（一）](https://blog.csdn.net/qinyushuang/article/details/46611709)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原理：利用队列（Qdisc）控制数据发送的方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./package_flow.png\" width=\"600\" alt=\"package flow\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "报文分组从输入网卡接收进来，经过路由查找，已确定是发给本机的，还是需要转发的。如果是发给本机的，就向上递交给上层协议，如TCP，如果是转发的就从输出网卡发出。***网络流量的控制通常发生在输出网卡处。*** 原因是我们无法控制自己网络之外的设备，因此在入口处控制流量较难。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 队列：每个网卡都与一个队列相联系，每当内核需要将报文分组从网卡发送出去，都会首先将该报文分组添加到该网卡所配置的队列中，由该队列决定报文分组的发送顺序。\n",
    "- 队列规则(queuing discipline, qdisc)：队列决定报文分组的发送顺序所依据的规则，即管理设备输入（ingress）和输出（egress）的算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 术语"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 队列规则(Queuing Discipline, qdisc)\n",
    "> 管理设备输入（ingress）和输出（egress）的算法\n",
    "- 根队列规则（Root Qdisc）\n",
    "> 根队列规则就是直接依附于设备的队列规则\n",
    "- 无类队列规则（Classless Qdisc）\n",
    "> 一个内部不包含可配置子类的队列规则\n",
    "- 分类队列规则（Classiful Qdisc）\n",
    "> 一个分类的队列规则内可以包含更多的类。其中每个类又可以进一步包含一个队列规则，这个队列规则可以是分类的，也可以是无类的\n",
    "- 类（Classes）\n",
    "> 一个分类的队列规则可以拥有很多类，类内包含队列规则\n",
    "- 分类器（Classifier）\n",
    "> 每个分类的队列规则都需要决定什么样的包使用什么类进行发送，分类器就是做这个的\n",
    "- 过滤器(Filter)\n",
    "> 分类是通过过滤器完成的，一个过滤器包含若干匹配条件，如果符合匹配条件，就按此过滤分类\n",
    "- 调度(Scheduling)\n",
    "> 在分类器的帮助下，一个队列规则可以裁定某些数据包可以排在其他数据之前发送，这种处理叫做“调度”\n",
    "- 整形(shaping)\n",
    "> 在一个数据包发送之前进行适当的延迟，一面超过实现规则好的最大速率，这种处理叫做整形，整形在egress处进行。习惯上，通过丢包来降速也经常被称为整形\n",
    "- 策略(Policing)\n",
    "> 通过延迟或是丢弃数据包来保证流量不超过事先规则的带宽。在Linux中，策略总是规定丢弃数据包而不是延迟，即不存在ingress队列\n",
    "- Work-Conserving（即到即发）\n",
    "> 对于一个work-conserving队列规则，如果得到一个数据包，他总是立刻对它进行分发。换句话说，只要网卡（egress队列规则）允许，他就不会延迟数据包的发送\n",
    "- non-work-conserving\n",
    "> 有些队列，比如令牌桶过滤器，可能需要暂时停止发包以限制带宽。也就是说它们有时候即使数据包需要处理，也可能拒绝发送。\n",
    "- 图\n",
    "> ![TC FLOW](./tc_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 无类队列规则\n",
    "    - pfifo_fast\n",
    "    - 令牌桶过滤器（TBF）\n",
    "    - 随机公平队列（SFQ）\n",
    "2. 分类的队列规则\n",
    "    - PRIO队列规则\n",
    "    - CBQ队列规则(太TM复杂)\n",
    "    - HTB队列规则（分层令牌桶）\n",
    "    - IMQ（Intermediate queueing device, 中介队列设备）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 无类队列规则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **对于无类队列规则来说，网卡对报文不进行类别划分，只进行流量整形，无类队列能够对接收到的数据重新编排，设置延迟和丢包。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **pfifo_fast：（硬性的缺省配置，不能用TC命令对它进行配置）**\n",
    "    * 此规则将收到的数据包按照FIFO的原则进行转发，不对数据包进行任何特殊处理。这个队列有三个频道（band）。FIFO规则应用于每一个频道。并且：如果0频道有数据包等待发送，1频道的包就不会被处理，1频道和2频道之间的关系也是如此。\n",
    "    * priomap:\n",
    "    此规则为内核规则，根据数据包的优先权情况，映射到相应的频道。这个映射过程是根据数据包的TOS(Type of Service)字节进行的。\n",
    "        - TOS字节的格式：<br/>\n",
    "            ![TOS](TOS.png)\n",
    "            ![TOS Bits](TOS_Bits.png)\n",
    "        - TOS字节的4个bits定义如下：<br/>\n",
    "            ![TOS Mean](TOS_mean.png)\n",
    "        - 频道(band)划分规则如下:<br/>\n",
    "            ![TOS Bands](TOS_bands.png)\n",
    "    * tx queuelen: \n",
    "        - 队列的长度来自网卡的配置，可用ifconfig和ip命令修改。<br> 如设置队列长度为10，执行：ifconfig eth0 txqueuelen 10（不能用tc命令设置这个）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **令牌桶过滤器(TBF, Token Bucket Filter)**\n",
    "    * ![TBF](./TBF.png)\n",
    "    * 只允许以不超过事先设定的速率到来的数据包通过，但可能允许短暂突发流量超过设定值。\n",
    "    * TBF的实现在于一个缓冲器（桶），该缓冲器（桶）不断地被一些叫做”令牌”的虚拟数据以特定速率(token rate)填充着。桶最重要的参数就是它的大小，也就是它能够存储令牌的数量。\n",
    "    * *每个到来的令牌从数据队列中收集一个数据包，然后从桶中被删除*。因此这个算法关联在两个流上--令牌流和数据流。\n",
    "    \n",
    "    * **令牌桶算法**\n",
    "        - 目的: 为了防止网络拥塞，限制流出网络的流量，使流量以比较均匀的速度向外发送，并且允许突发数据的发送。\n",
    "        - 大小固定的令牌桶可以以恒定的速率源源不断地产生令牌，知道令牌充满整个令牌桶，则后面产生的令牌就会被丢弃。即任何时候的令牌数量都不会超过令牌桶的最大容量。\n",
    "        \n",
    "        - 三种情况：\n",
    "            1. **V(数据流) = V(令牌流)，每个到来的数据包都能对应一个令牌，然后无延迟地通过队列；**\n",
    "            2. **V(数据流) < V(令牌流)，通过队列的数据只能消耗掉一部分令牌，剩下的令牌会在令牌桶中积累下来，知道令牌桶被装满。剩下的令牌可以再需要以高于令牌流速率发送数据流的时候消耗掉，这种情况即为突发传输；**\n",
    "            3. **V(数据流) > V(令牌流)， 意味着桶内令牌会被很快耗尽，导致TBF中断一段时间，称为越限(overlimit)。如果数据包持续到来，将发生丢包。**\n",
    "        \n",
    "        - 参数使用：\n",
    "            1. limit/latency\n",
    "                > limit确定最多有多少数据（字节数）在队列中等待令牌。你也可以通过设置latency来指定这个参数，latency参数确定了一个包在TBF中等待传输的最长等待时间。两者计算决定桶的大小、速率和峰值速率。\n",
    "            \n",
    "            2. burst/buffer/maxburst\n",
    "                > 桶的大小，以字节计。这个参数指定了最多可以有多少个令牌能够即刻被使用。通常，管理的带宽越大，需要的缓冲器就越大。在Intel体系上，10Mbit/s的速率需要至少10k字节的缓冲区才能达到期望的速率。如果你的缓冲区太小，就会导致到达的令牌没有地方放（桶满了），这会导致潜在的丢包。\n",
    "\n",
    "            3. MPU\n",
    "                > 一个零长度的包并不是不耗费带宽。比如以太网，数据帧不会小于64字节。MPU(Minimum Packet Unit，最小分组单元)决定了令牌的最低消耗。\n",
    "\n",
    "            4. rate\n",
    "                > 速度操纵杆，rate = limit/latency\n",
    "            \n",
    "            5. peakrate（峰值速率）\n",
    "                >  如果有可用的令牌，数据包一旦到来就会立刻被发送出去，就像光速一样。那可能并不是你希望的，特别是你有一个比较大的桶的时候。峰值速率可以用来指定令牌以多快的速度被删除。用书面语言来说，就是：**释放一个数据包，然后等待足够的时间后再释放下一个**。我们通过计算等待时间来控制峰值速率。例如：UNIX定时器的分辨率是10毫秒，如果平均包长10kb，我们的峰值速率被限制在了1Mbps。\n",
    "            \n",
    "        - 配置范例：\n",
    "            ```sh\n",
    "            tc qdisc add dev ppp0 root tbf rate 220kbit latency 50ms burst 1540\n",
    "            ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **随机公平队列（Stochastic Fairness Queueing, SFQ）**\n",
    "    * SFQ的关键词是**会话**和**流**，主要针对一个TCP会话或者UDP流。流量被分成相当多数量的FIFO队列，每一个队列对应一个会话。数据按照简单轮转的方式发送，每个会话都按顺序得到发送机会。\n",
    "    * SFQ中随机的含义为，它并不是真的为每一个会话创建一个队列，而是使用一个散列算法，把所有会话映射到有限的几个队列中去。因为使用了散列，所以可能多个会话分配到同一个队列中，从而需要共享发包的机会，也就是共享带宽。为了不让这种效应太明显，SFQ会频繁地改变散列算法，以便把这种效应控制在几秒钟之内。\n",
    "    * **注意：只有当你的输出网卡确实已经挤满了的时候，SFQ才会起作用！否则在你的Linux机器中根本就不会有队列，SFQ也就不会起作用。**\n",
    "    * **在SFQ中数据包进出路由器没有任何延迟, 它只是一种相对公平的调度方法，并不能控制流量。**\n",
    "    * 参数使用\n",
    "        1. perturb: 多少秒后重新设置一次散列算法。如果取消设置，散列算法将永远不会重新设置。10秒应该是一个合适的值。\n",
    "        2. quantum: 一个流至少要传输多少字节后切换到下一个队列。缺省设置一个最大包的长度（MTU的大小）。不要设置这个数值低于MTU！\n",
    "    * 配置范例\n",
    "    ```sh\n",
    "    tc qdisc add dev ppp0 root sfq perturb 10\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 分类的队列规则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 分类的队列规则及其类中的数据流向\n",
    "   * 数据包进入一个分类的队列规则后，过滤器会根据设定好的过滤条件返回一个决定，队列规则根据这个决定把数据包送入相应的类进行排队。每个子类可以再次使用它们自己的过滤器进一步分类，直到不需要分类为止，数据包才进入该类包含的队列规则等待处理。 \n",
    "   * **注意： 分类器是从队列规则内部调用的，它决定的是使用哪个类进行发包**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 队列规则中的语义\n",
    "    * 每个网卡都有一个出口“根队列规则”，缺省情况下是pfifo_fast队列规则，每个队列规则都指定一个句柄（就是队列代号），一遍以后的配置语句能够引用这个队列规则。\n",
    "    * 队列规则的句柄有两个部分：一个主号码和一个次号码。习惯上把根队列规则称为\"1:\"，等价于\"1:0\"。**队列规则的次号码永远是0**\n",
    "    * 类的主号码必须与它们父辈的主号码一致\n",
    "    * 使用过滤器进行分类：\n",
    "        > ![Filter](./filter.png)\n",
    "    * 数据包是在根队列规则处入队和出队的，而内核只与“根”打交道\n",
    "    * 数据包是如何出队的？\n",
    "        > 当内核决定把一个数据包发给网卡的时候，根队列规则1:会得到一个出队请求，然后把它传给1:1，然后依次传给10:,11:和12:,然后试图从它们中进行出队操作。也就是说，内核会遍历整棵树，因为只有12:2才有这个数据包。换句话说，类及兄弟仅仅与其父队列规则进行交谈，而不会与网卡进行交谈。只有根队列规则才能由内核操作进行出队操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **PRIO队列规则**\n",
    "    * 首先，PRIO规则并不进行流量整形，它仅仅根据配置的过滤器把流量进一步细分。\n",
    "    * PRIO其实就是一个加强版的pfifo_fast规则，区别在于每个band都是一个单独的类，而不是简单的FIFO原则。\n",
    "    * 数据包进入PRIO队列规则后，根据filter的设置选择一个类，缺省情况下有三个类，不过这三个类仅包含FIFO原则。\n",
    "    * 和pfifo_fast一样，总是先处理band小的类，只有当band小的队列中没有数据包之后才处理band标号大的类。\n",
    "    * PRIO不进行流量整形，所以和SFQ一样，要么在egress队列满了之后使用它，要么将其包含在一个整形队列规则的内部\n",
    "    * PRIO队列规则是一种Work-Conserving的调度\n",
    "    * PRIO参数使用：\n",
    "        - bands:\n",
    "            > 创建频道的数目。每个频道实际上就是一个类。如果你修改了这个数值，你必须同是修改：\n",
    "        - priomap：\n",
    "            > 如果不给TC提供任何过滤器，PRIO队列规则将参考TC_PRIO的优先级来决定如何给数据包入队，它的行为和pfifo_fast队列规则（FIFO）一样。<br/>\n",
    "            > 其实频道就是类，缺省情况下命名为\"主标号:1\"到\"主标号:3\"。如果你的PRIO队列规则是\"12:\",把数据包过滤到\"12:1\"将得到最高优先级，注意：**0频道的次标号是1；1频道的次标号是2，以此类推。**\n",
    "    * 配置范例：\n",
    "        - ![PRIO EXAMPLE](./PRIO_example.png)\n",
    "            大批量数据使用30：交互式数据使用20：或10：\n",
    "        - 命令如下：\n",
    "        ```sh\n",
    "        # 此命令立即创建了类1:1, 1:2, 1:3\n",
    "        tc qdisc add dev eth0 root handle 1: prio\n",
    "        \n",
    "        # 0号频道使用sfq队列规则（10号队列规则）\n",
    "        tc qdisc add dev eth0 parent 1:1 handle 10: sfq\n",
    "        # 1号频道使用tbf队列规则（20号队列规则）\n",
    "        tc qdisc add dev eth0 parent 1:2 handle 20: tbf rate 20kbit buffer 1600 limit 3000\n",
    "        # 2号频道使用sfq队列规则（30号队列规则）\n",
    "        tc qdisc add dev eth0 parent 1:3 handle 30: sfq\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **HTB（Hierarchical Token Bucket, 分层的令牌桶）**\n",
    "    * HTB能够满足这样一种情况: 你有一个固定速率的链路，希望分割给多种不同的用途使用。为每种用途做出带宽承诺兵实现定量带宽借用。\n",
    "    * ![HTB](./HTB.png)\n",
    "    * 分类：\n",
    "        - HTB中的分类其实就是分层拓扑中的节点，不同的类有不同的带宽配置，详细的配置如下：\n",
    "        ```sh\n",
    "          tc class add ... htb rate R1 burst B1 [prio P] [slot S] [pslot PS]\n",
    "                          [ceil R2] [cburst B2] [mtu MTU] [quantum Q]\n",
    "             rate     rate allocated to this class (class can still borrow)\n",
    "             burst    max bytes burst which can be accumulated during idle period {computed}\n",
    "             ceil     definite upper class rate (no borrows){rate}\n",
    "             cburst   burst but for ceil {computed}\n",
    "             mtu      max packet size we create rate map for{1600}\n",
    "             prio     priority of leaf; lower are served first {0}\n",
    "             quantum  how much bytes to serve from leaf at once {use r2q}\n",
    "        ```\n",
    "        \n",
    "        - 其中有几个比较关键的概念：\n",
    "            1. rate：带宽速率，单位是bit/mbit/kbit/gbit或者bps/mbps/kbps/gbps\n",
    "            2. ceil：上限带宽速率，单位与rate一致，ceil是实现HTB的流量租借的重要参数\n",
    "            3. burst：突发数据包大小\n",
    "            4. cburst：租借情况下的突发数据包大小<br/>\n",
    "            其中rate和ceil可以分别对应到Matrix中的软限和硬限，burst主要用来控制流量整形的精确度\n",
    "    \n",
    "    * 过滤器:\n",
    "        - 内核只和根打交道，数据包是在根队列规定处入队和出队的：\n",
    "            1. 数据包进入队列时，自上而下进入拓扑中的某个子节点\n",
    "            2. 当内核决定把一个数据包发给网卡的时候，根队列规定会得到一个出队请求，然后它会把请求以递归的方式发送给所有孩子，最终只有叶子节点才能执行真正的数据包出队操作\n",
    "            3. 为了决定用哪个类处理数据包，必须调用所谓的分类选择器进行选择，这就是TC中的filter概念，filter会根据自身设定的规则，已决定当前数据包进入哪一个子节点，目前tc支持的分类器有fw,u32,route,cgroup等，它们主要的区别是：\n",
    "                - fw根据防火墙如何对这个数据包做标记进行判断\n",
    "                - u32根据数据包中的各个字段进行判断\n",
    "                - route根据数据如何被路由进行判断\n",
    "                - cgroup与net_cls子系统结合，根据进程pid来判断\n",
    "       \n",
    "        - 配置范例\n",
    "           > 此例子把WEB服务器的流量控制为5mbit， SMTP流量控制在3mbit。而且二者一共不得超过6mbkit，互相之间允许互借带宽，网卡为100Mbps\n",
    "           ```sh\n",
    "           tc qdisc add dev eth0 root handle 1: htb default 30\n",
    "           tc class add dev eth0 parent 1: classid 1:1 htb rate 6mbit burst 15k\n",
    "           tc class add dev eth0 parent 1:1 classid 1:10 htb rate 5mbit burst 15k\n",
    "           tc class add dev eth0 parent 1:1 classid 1:20 htb rare 3mbit ceil 6mbit burst 15k\n",
    "           tc class add dev eth0 parent 1:1 classid 1:30 htb rare 1lbit ceil 6mbit burst 15k\n",
    "           ```\n",
    "        - 常用过滤命令\n",
    "            > 这里列出的绝大多的数命令都根据这个命令改编而来：\n",
    "            ```sh\n",
    "            tc filter add dev eth0 parent 1: protocol ip prio 1 u32\n",
    "            ```\n",
    "            \n",
    "            1. 根据源/目的地址:\n",
    "                > 源地址段 match ip ssrc 1.2.3.0/24\n",
    "                > 目的地址段 match ip dst 4.3.2.0/24\n",
    "            2. 根据源/目的端口，所有IP协议\n",
    "                > 源端口 match ip sport 80 0xffff\n",
    "                > 目的端口 match ip dport 80 0xffff\n",
    "            3. 根据IP协议（tcp, udp, icmp, gre, ipsec）\n",
    "                > 使用/etc/protocols所指定的数字\n",
    "                > 比如: icmp是 match ip protocol 1 0xff\n",
    "            4. 根据fwark(防火墙标记功能)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
